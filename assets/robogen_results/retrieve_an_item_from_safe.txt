========== Generated task name and description ==========
retrieve an item from safe
The robot arm is tasked with opening the safe, retrieving an item from inside it, and then closing the safe again

========== Scene configuration ==========
- use_table: true
- center: (1.0, 0.2, 0)
  lang: a common safe
  name: Safe
  on_table: false
  path: safe.urdf
  size: 1.0
  type: urdf
- center: (1.0, 0.2, 0.3)
  lang: a gold bar
  name: Item
  on_table: false
  path: gold_bar.obj
  size: 0.15
  type: mesh
- joint_1: '0'
  set_joint_angle_object_name: Safe
- spatial_relationships:
  - in, item, safe, link_2
- center: (0.8, -0.5, 0)
  lang: a common office chair
  name: Chair
  on_table: false
  path: chair.obj
  size: 0.8
  type: urdf
- center: (0.5, 0.5, 0)
  lang: a common laptop
  name: Laptop
  on_table: true
  path: laptop.obj
  size: 0.3
  type: urdf
- center: (0.7, 0.7, 0)
  lang: a common desk lamp
  name: Lamp
  on_table: true
  path: lamp.obj
  size: 0.3
  type: urdf

========== Task decompositions (substep: learning algorithm type, action space) ==========
grasp the safe door: primitive, none
open the safe door: reward, delta_translation
grasp the item: primitive, none
move the item out of the safe: reward, target_location
grasp the safe door again: primitive, none
close the safe door: reward, delta_translation
grasp the safe knob: primitive, none
lock the safe by rotating the knob back: reward, delta_translation

========== Reward and primitives ==========
- grasp_the_safe_door
    def execute(self):
        rgbs, final_state = grasp_object_link(self, "Safe", "link_0")  
        success = check_grasped(self, "Safe", "link_0")

        return rgbs, final_state, success

- open_the_safe_door
    def _compute_reward(self):
        # This reward encourages the end-effector to stay near the door to grasp it.
        eef_pos = get_eef_pos(self)[0]
        door_pos = get_link_state(self, "Safe", "link_0")
        reward_near = -np.linalg.norm(eef_pos - door_pos)
        
        # Get the joint state of the door. The semantics and the articulation tree show that joint_0 connects link_0 and is the joint that controls the rotation of the door.
        joint_angle = get_joint_state(self, "Safe", "joint_0") 
        # The reward is the negative distance between the current joint angle and the joint angle when the door is fully open (upper limit).
        joint_limit_low, joint_limit_high = get_joint_limit(self, "Safe", "joint_0")
        diff = np.abs(joint_angle - joint_limit_high)
        reward_joint =  -diff
        
        reward = reward_near + 5 * reward_joint
        
        success = diff < 0.35 * (joint_limit_high - joint_limit_low)
        
        return reward, success

- grasp_the_item
    def execute(self):
        rgbs, final_state = grasp_object(self, "Item")
        success = check_grasped(self, "Item")

        return rgbs, final_state, success

- move_the_item_out_of_the_safe
    def _compute_reward(self):
        # Get the current item position
        item_position = get_position(self, "Item")
        
        # This reward encourages the end-effector to stay near the item
        eef_pos = get_eef_pos(self)[0]
        reward_near = -np.linalg.norm(eef_pos - item_position)
        
        # The reward is to encourage the robot to grasp the item and move the item to be on the table. 
        table_bbox_low, table_bbox_high = get_bounding_box(self, "init_table") # the table is referred to as "init_table" in the simulator. 
        table_bbox_range = table_bbox_high - table_bbox_low
        
        # target location is to put the item at a random location on the table
        target_location = np.zeros(3)
        target_location[0] = table_bbox_low[0] + 0.2 * table_bbox_range[0] # 0.2 is a random chosen number, any number in [0, 1] should work
        target_location[1] = table_bbox_low[1] + 0.3 * table_bbox_range[1] # 0.3 is a random chosen number, any number in [0, 1] should work
        target_location[2] = table_bbox_high[2] # the height should be the table height
        diff = np.linalg.norm(item_position - target_location)
        reward_distance = -diff

        reward = reward_near + 5 * reward_distance
        
        success = diff < 0.06
        
        return reward, success

- grasp_the_safe_door_again
    def execute(self):
        rgbs, final_state = grasp_object_link(self, "Safe", "link_0")
        success = check_grasped(self, "Safe", "link_0") 

        return rgbs, final_state, success

- close_the_safe_door
    def _compute_reward(self):
        # This reward encourages the end-effector to stay near the door
        eef_pos = get_eef_pos(self)[0]
        door_pos = get_link_state(self, "Safe", "link_0")
        reward_near = -np.linalg.norm(eef_pos - door_pos)
        
        # Get the joint state of the door. The semantics and the articulation tree show that joint_0 connects link_0 and is the joint that controls the rotation of the door.
        joint_angle = get_joint_state(self, "Safe", "joint_0") 
        # The reward encourages the robot to make joint angle of the door to be the lower limit to close it.
        joint_limit_low, joint_limit_high = get_joint_limit(self, "Safe", "joint_0")
        diff = np.abs(joint_limit_low - joint_angle)
        reward_joint =  -diff
        
        reward = reward_near + 5 * reward_joint
        
        success = diff < 0.1 * (joint_limit_high - joint_limit_low) # for closing, we think 10 percent is enough     
        
        return reward, success

- grasp_the_safe_knob
    def execute(self):
        rgbs, final_state = grasp_object_link(self, "Safe", "link_1")  
        success = check_grasped(self, "Safe", "link_1")

        return rgbs, final_state, success

- lock_the_safe_by_rotating_the_knob_back
    def _compute_reward(self):
        # This reward encourages the end-effector to stay near the knob to grasp it.
        eef_pos = get_eef_pos(self)[0]
        knob_pos = get_link_state(self, "Safe", "link_1")
        reward_near = -np.linalg.norm(eef_pos - knob_pos)
        
        # Get the joint state of the door. The semantics and the articulation tree show that joint_0 connects link_0 and is the joint that controls the rotation of the door.
        joint_angle = get_joint_state(self, "Safe", "joint_0") 
        # The reward encourages the robot to make joint angle of the door to be the lower limit to close it.
        joint_limit_low, joint_limit_high = get_joint_limit(self, "Safe", "joint_0")
        diff = np.abs(joint_limit_low - joint_angle)
        reward_door =  -diff

        # Get the joint state of the knob. The semantics and the articulation tree show that joint_1 connects link_1 and is the joint that controls the rotation of the knob.
        joint_angle = get_joint_state(self, "Safe", "joint_1") 
        # The reward is the negative distance between the current joint angle and the joint angle when the knob is fully rotated back (lower limit).
        joint_limit_low, joint_limit_high = get_joint_limit(self, "Safe", "joint_1")
        diff = np.abs(joint_angle - joint_limit_low)
        reward_joint =  -diff
        
        reward = reward_near + 5 * reward_joint # + 0.05 * reward_door
        
        success = diff < 0.1 * (joint_limit_high - joint_limit_low)
        
        return reward, success

